{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45c01a7",
   "metadata": {},
   "source": [
    "# **Using a singular model-structure to predict the data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb8551",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77ec88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Models\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, GlobalAveragePooling1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n",
    "\n",
    "datasets_folder = './datasets'\n",
    "figsize = (20,4)\n",
    "verbosity = 0\n",
    "\n",
    "metrics = [root_mean_squared_error, r2_score, mean_absolute_error, huber]\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d231c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_data = read_and_preprocess_dataset(datasets_folder, 'pollution', v=verbosity)\n",
    "traffic_data = read_and_preprocess_dataset(datasets_folder, 'traffic', v=verbosity, radius=2)\n",
    "weather_data = read_and_preprocess_dataset(datasets_folder, 'weather', v=verbosity)\n",
    "\n",
    "stations = list(pollution_data.keys())\n",
    "agents = list(set(agent for station in stations for agent in pollution_data[station].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a0ad5",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4ea51",
   "metadata": {},
   "source": [
    "- Traffic data is per-station, but it should be normalized globally. To do so we first merge the data, we apply normalization, and later use the date and the statino to merge the data with the pollutants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c86605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic_value    0\n",
      "Station          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_value</th>\n",
       "      <th>Station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>27629.0</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>44430.0</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>40318.0</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 03:00:00</th>\n",
       "      <td>25661.0</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 04:00:00</th>\n",
       "      <td>15906.0</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 19:00:00</th>\n",
       "      <td>19786.0</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 20:00:00</th>\n",
       "      <td>16239.0</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 21:00:00</th>\n",
       "      <td>7226.0</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 22:00:00</th>\n",
       "      <td>5568.0</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 23:00:00</th>\n",
       "      <td>4587.0</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157824 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Traffic_value              Station\n",
       "Date                                                   \n",
       "2019-01-01 00:00:00        27629.0  GIARDINI MARGHERITA\n",
       "2019-01-01 01:00:00        44430.0  GIARDINI MARGHERITA\n",
       "2019-01-01 02:00:00        40318.0  GIARDINI MARGHERITA\n",
       "2019-01-01 03:00:00        25661.0  GIARDINI MARGHERITA\n",
       "2019-01-01 04:00:00        15906.0  GIARDINI MARGHERITA\n",
       "...                            ...                  ...\n",
       "2024-12-31 19:00:00        19786.0         VIA CHIARINI\n",
       "2024-12-31 20:00:00        16239.0         VIA CHIARINI\n",
       "2024-12-31 21:00:00         7226.0         VIA CHIARINI\n",
       "2024-12-31 22:00:00         5568.0         VIA CHIARINI\n",
       "2024-12-31 23:00:00         4587.0         VIA CHIARINI\n",
       "\n",
       "[157824 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_traffic_data = pd.concat(\n",
    "    [df.assign(Station=key) for key, df in traffic_data.items()]\n",
    ")\n",
    "print(merged_traffic_data.resample('1h').max().isna().sum())\n",
    "merged_traffic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe7dc1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_value</th>\n",
       "      <th>Station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>-0.366692</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>0.111507</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>-0.005531</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 03:00:00</th>\n",
       "      <td>-0.422706</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 04:00:00</th>\n",
       "      <td>-0.700359</td>\n",
       "      <td>GIARDINI MARGHERITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 19:00:00</th>\n",
       "      <td>-0.589924</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 20:00:00</th>\n",
       "      <td>-0.690881</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 21:00:00</th>\n",
       "      <td>-0.947414</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 22:00:00</th>\n",
       "      <td>-0.994605</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 23:00:00</th>\n",
       "      <td>-1.022526</td>\n",
       "      <td>VIA CHIARINI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157824 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Traffic_value              Station\n",
       "Date                                                   \n",
       "2019-01-01 00:00:00      -0.366692  GIARDINI MARGHERITA\n",
       "2019-01-01 01:00:00       0.111507  GIARDINI MARGHERITA\n",
       "2019-01-01 02:00:00      -0.005531  GIARDINI MARGHERITA\n",
       "2019-01-01 03:00:00      -0.422706  GIARDINI MARGHERITA\n",
       "2019-01-01 04:00:00      -0.700359  GIARDINI MARGHERITA\n",
       "...                            ...                  ...\n",
       "2024-12-31 19:00:00      -0.589924         VIA CHIARINI\n",
       "2024-12-31 20:00:00      -0.690881         VIA CHIARINI\n",
       "2024-12-31 21:00:00      -0.947414         VIA CHIARINI\n",
       "2024-12-31 22:00:00      -0.994605         VIA CHIARINI\n",
       "2024-12-31 23:00:00      -1.022526         VIA CHIARINI\n",
       "\n",
       "[157824 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_traffic_data = merged_traffic_data.copy()\n",
    "scaled_traffic_data['Traffic_value'] = scaler.fit_transform(merged_traffic_data[['Traffic_value']])\n",
    "scaled_traffic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258542d2",
   "metadata": {},
   "source": [
    "- weather data is global, we only have to normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c3d415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAVG          0\n",
      "PREC          0\n",
      "RHAVG         0\n",
      "RAD           0\n",
      "W_SCAL_INT    0\n",
      "W_VEC_DIR     0\n",
      "LEAFW         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PREC</th>\n",
       "      <th>RHAVG</th>\n",
       "      <th>RAD</th>\n",
       "      <th>W_SCAL_INT</th>\n",
       "      <th>W_VEC_DIR</th>\n",
       "      <th>LEAFW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>-1.746206</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.318018</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.045012</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>-1.828166</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.382433</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>-1.781331</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.288289</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.088148</td>\n",
       "      <td>-0.523708</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 03:00:00</th>\n",
       "      <td>-1.816457</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.278379</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.308868</td>\n",
       "      <td>-0.171911</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 04:00:00</th>\n",
       "      <td>-1.746206</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.020722</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.308868</td>\n",
       "      <td>-0.357363</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 19:00:00</th>\n",
       "      <td>-1.113941</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>0.743245</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-0.260449</td>\n",
       "      <td>0.441767</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 20:00:00</th>\n",
       "      <td>-0.985147</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>0.525228</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>0.236170</td>\n",
       "      <td>0.355222</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 21:00:00</th>\n",
       "      <td>-1.055398</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>0.629282</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>0.236170</td>\n",
       "      <td>0.345107</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 22:00:00</th>\n",
       "      <td>-1.043690</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>0.559913</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>0.953508</td>\n",
       "      <td>0.308016</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 23:00:00</th>\n",
       "      <td>-1.043690</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>0.723426</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>1.229408</td>\n",
       "      <td>0.288909</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52608 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         TAVG      PREC     RHAVG       RAD  W_SCAL_INT  \\\n",
       "Date                                                                      \n",
       "2019-01-01 00:00:00 -1.746206 -0.136824  1.318018 -0.661589   -1.419227   \n",
       "2019-01-01 01:00:00 -1.828166 -0.136824  1.382433 -0.661589   -1.419227   \n",
       "2019-01-01 02:00:00 -1.781331 -0.136824  1.288289 -0.661589   -1.088148   \n",
       "2019-01-01 03:00:00 -1.816457 -0.136824  1.278379 -0.661589   -1.308868   \n",
       "2019-01-01 04:00:00 -1.746206 -0.136824  1.020722 -0.661589   -1.308868   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2024-12-31 19:00:00 -1.113941 -0.136824  0.743245 -0.661589   -0.260449   \n",
       "2024-12-31 20:00:00 -0.985147 -0.136824  0.525228 -0.661589    0.236170   \n",
       "2024-12-31 21:00:00 -1.055398 -0.136824  0.629282 -0.661589    0.236170   \n",
       "2024-12-31 22:00:00 -1.043690 -0.136824  0.559913 -0.661589    0.953508   \n",
       "2024-12-31 23:00:00 -1.043690 -0.136824  0.723426 -0.661589    1.229408   \n",
       "\n",
       "                     W_VEC_DIR     LEAFW  \n",
       "Date                                      \n",
       "2019-01-01 00:00:00   0.045012 -0.465156  \n",
       "2019-01-01 01:00:00   0.846389 -0.465156  \n",
       "2019-01-01 02:00:00  -0.523708 -0.465156  \n",
       "2019-01-01 03:00:00  -0.171911 -0.465156  \n",
       "2019-01-01 04:00:00  -0.357363 -0.465156  \n",
       "...                        ...       ...  \n",
       "2024-12-31 19:00:00   0.441767 -0.465156  \n",
       "2024-12-31 20:00:00   0.355222 -0.465156  \n",
       "2024-12-31 21:00:00   0.345107 -0.465156  \n",
       "2024-12-31 22:00:00   0.308016 -0.465156  \n",
       "2024-12-31 23:00:00   0.288909 -0.465156  \n",
       "\n",
       "[52608 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_weather_data = weather_data.copy()\n",
    "scaled_weather_data[:] = scaler.fit_transform(scaled_weather_data)\n",
    "print(scaled_weather_data.resample('1h').max().isna().sum())\n",
    "scaled_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564b4ad",
   "metadata": {},
   "source": [
    "- We can now divide the traffic per station, merge them with the weather and create the input sequences. For every agent of the same station, the input data is the same.\n",
    "    \n",
    "    We might also want to consider adding new informations, like the date encodings and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "068a6abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GIARDINI MARGHERITA'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traffic_value    0\n",
       "TAVG             0\n",
       "PREC             0\n",
       "RHAVG            0\n",
       "RAD              0\n",
       "W_SCAL_INT       0\n",
       "W_VEC_DIR        0\n",
       "LEAFW            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_value</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PREC</th>\n",
       "      <th>RHAVG</th>\n",
       "      <th>RAD</th>\n",
       "      <th>W_SCAL_INT</th>\n",
       "      <th>W_VEC_DIR</th>\n",
       "      <th>LEAFW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>-0.366692</td>\n",
       "      <td>-1.746206</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.318018</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.045012</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>0.111507</td>\n",
       "      <td>-1.828166</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.382433</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>-0.005531</td>\n",
       "      <td>-1.781331</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.288289</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.088148</td>\n",
       "      <td>-0.523708</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Traffic_value      TAVG      PREC     RHAVG       RAD  \\\n",
       "Date                                                                         \n",
       "2019-01-01 00:00:00      -0.366692 -1.746206 -0.136824  1.318018 -0.661589   \n",
       "2019-01-01 01:00:00       0.111507 -1.828166 -0.136824  1.382433 -0.661589   \n",
       "2019-01-01 02:00:00      -0.005531 -1.781331 -0.136824  1.288289 -0.661589   \n",
       "\n",
       "                     W_SCAL_INT  W_VEC_DIR     LEAFW  \n",
       "Date                                                  \n",
       "2019-01-01 00:00:00   -1.419227   0.045012 -0.465156  \n",
       "2019-01-01 01:00:00   -1.419227   0.846389 -0.465156  \n",
       "2019-01-01 02:00:00   -1.088148  -0.523708 -0.465156  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'PORTA SAN FELICE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traffic_value    0\n",
       "TAVG             0\n",
       "PREC             0\n",
       "RHAVG            0\n",
       "RAD              0\n",
       "W_SCAL_INT       0\n",
       "W_VEC_DIR        0\n",
       "LEAFW            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_value</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PREC</th>\n",
       "      <th>RHAVG</th>\n",
       "      <th>RAD</th>\n",
       "      <th>W_SCAL_INT</th>\n",
       "      <th>W_VEC_DIR</th>\n",
       "      <th>LEAFW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>0.011177</td>\n",
       "      <td>-1.746206</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.318018</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.045012</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>0.599669</td>\n",
       "      <td>-1.828166</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.382433</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>0.316324</td>\n",
       "      <td>-1.781331</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.288289</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.088148</td>\n",
       "      <td>-0.523708</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Traffic_value      TAVG      PREC     RHAVG       RAD  \\\n",
       "Date                                                                         \n",
       "2019-01-01 00:00:00       0.011177 -1.746206 -0.136824  1.318018 -0.661589   \n",
       "2019-01-01 01:00:00       0.599669 -1.828166 -0.136824  1.382433 -0.661589   \n",
       "2019-01-01 02:00:00       0.316324 -1.781331 -0.136824  1.288289 -0.661589   \n",
       "\n",
       "                     W_SCAL_INT  W_VEC_DIR     LEAFW  \n",
       "Date                                                  \n",
       "2019-01-01 00:00:00   -1.419227   0.045012 -0.465156  \n",
       "2019-01-01 01:00:00   -1.419227   0.846389 -0.465156  \n",
       "2019-01-01 02:00:00   -1.088148  -0.523708 -0.465156  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'VIA CHIARINI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Traffic_value    0\n",
       "TAVG             0\n",
       "PREC             0\n",
       "RHAVG            0\n",
       "RAD              0\n",
       "W_SCAL_INT       0\n",
       "W_VEC_DIR        0\n",
       "LEAFW            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_value</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>PREC</th>\n",
       "      <th>RHAVG</th>\n",
       "      <th>RAD</th>\n",
       "      <th>W_SCAL_INT</th>\n",
       "      <th>W_VEC_DIR</th>\n",
       "      <th>LEAFW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>-0.913856</td>\n",
       "      <td>-1.746206</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.318018</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.045012</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>-0.724523</td>\n",
       "      <td>-1.828166</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.382433</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.419227</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 02:00:00</th>\n",
       "      <td>-0.787369</td>\n",
       "      <td>-1.781331</td>\n",
       "      <td>-0.136824</td>\n",
       "      <td>1.288289</td>\n",
       "      <td>-0.661589</td>\n",
       "      <td>-1.088148</td>\n",
       "      <td>-0.523708</td>\n",
       "      <td>-0.465156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Traffic_value      TAVG      PREC     RHAVG       RAD  \\\n",
       "Date                                                                         \n",
       "2019-01-01 00:00:00      -0.913856 -1.746206 -0.136824  1.318018 -0.661589   \n",
       "2019-01-01 01:00:00      -0.724523 -1.828166 -0.136824  1.382433 -0.661589   \n",
       "2019-01-01 02:00:00      -0.787369 -1.781331 -0.136824  1.288289 -0.661589   \n",
       "\n",
       "                     W_SCAL_INT  W_VEC_DIR     LEAFW  \n",
       "Date                                                  \n",
       "2019-01-01 00:00:00   -1.419227   0.045012 -0.465156  \n",
       "2019-01-01 01:00:00   -1.419227   0.846389 -0.465156  \n",
       "2019-01-01 02:00:00   -1.088148  -0.523708 -0.465156  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "station_data = {\n",
    "    station: pd.merge(\n",
    "        scaled_traffic_data[scaled_traffic_data['Station'] == station].drop(columns=['Station']),\n",
    "        scaled_weather_data,\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "    for station in stations\n",
    "}\n",
    "for station in station_data:\n",
    "    display(station, station_data[station].resample('1h').max().isna().sum(), station_data[station].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96cd32",
   "metadata": {},
   "source": [
    "Sequences should also be indexed with the date of the last element, so that merging with the agent will be easier. To do so, we simply use one variable for the sequences and one for the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94ac2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_time_steps = 3\n",
    "daily_time_steps = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sliding_windows = {}\n",
    "for station in station_data:\n",
    "    sequences, dates = create_sequences(\n",
    "        station_data[station],\n",
    "        pd.Series(station_data[station].index, index=station_data[station].index),\n",
    "        time_steps=hourly_time_steps\n",
    "    )\n",
    "    dates = dates.index.tolist()\n",
    "    station_sliding_windows[station] = sequences, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4f0b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sequences = {}\n",
    "for station in station_data:\n",
    "    sequences, dates = create_sequences(\n",
    "        station_data[station],\n",
    "        pd.Series(station_data[station].index, index=station_data[station].index),\n",
    "        time_steps=daily_time_steps,\n",
    "        sliding_window=False\n",
    "    ) \n",
    "    station_sequences[station] = sequences, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27730f",
   "metadata": {},
   "source": [
    "- At this point we should create the actual train data composed of sequence + agentID and the target values. We could of course create an huge dataset and directly use it, but I dont know if it is too big and with too many repeated values (sequences are repeated for each agent and each sequence share some rows).\n",
    "\n",
    "    <!-- For now, I simply try to make a dataset made of `in=(station_date, sequence)`, `x=(station_date, agent)` and `y=(target)`. I don't want the model to learn station and agent.  -->\n",
    "    The model should be able to take a batch and use the dates to access the sequences, create the embeddings, and use the agent to predict the concentration using the correct classification head. Since that we have to map a dict would be suitable for fast data access.\n",
    "\n",
    "- Also, at this point we have to divide hourly and daily agents because the embedding we are trying to create for the stations can be quite different. For example if we have to predict an hourly pollutant, we might want to use 3/5 prev. hours. For daily agents instead, the whole 24 hours might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f4b103ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(station, date):\n",
    "    return (station+' '+str(date)).replace(' ','_')\n",
    "\n",
    "hourly_input_data = {}\n",
    "daily_input_data = {}\n",
    "for station in station_sequences:\n",
    "    for seq, date in zip(station_sliding_windows[station][0], station_sliding_windows[station][1]):\n",
    "        hourly_input_data[create_key(station, date)] = seq\n",
    "    for seq, date in zip(station_sequences[station][0], station_sequences[station][1]):\n",
    "        daily_input_data[create_key(station, date)] = seq\n",
    "\n",
    "h_keys = []\n",
    "h_agentsIDs = []\n",
    "h_values = []\n",
    "\n",
    "d_keys = []\n",
    "d_agentsIDs = []\n",
    "d_values = []\n",
    "\n",
    "for station in pollution_data:\n",
    "    for agent in pollution_data[station]:\n",
    "        for target, date in zip(pollution_data[station][agent]['Agent_value'], pollution_data[station][agent].index):\n",
    "            if agent in ('PM10', 'PM2.5'):\n",
    "                d_keys.append(create_key(station, date))\n",
    "                d_agentsIDs.append(agents.index(agent))\n",
    "                d_values.append(target)\n",
    "            else: \n",
    "                h_keys.append(create_key(station, date))\n",
    "                h_agentsIDs.append(agents.index(agent))\n",
    "                h_values.append(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75079162",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923a09b",
   "metadata": {},
   "source": [
    "- The model will use a TensorFlow DataSet for better something (something's gotta be better right?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a9d2",
   "metadata": {},
   "source": [
    "Remove rows if there is no input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f10699b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for hourly data\n",
    "valid_h_indices = [i for i, k in enumerate(h_keys) if k in hourly_input_data]\n",
    "h_keys       = [h_keys[i]       for i in valid_h_indices]\n",
    "h_agentsIDs  = [h_agentsIDs[i]  for i in valid_h_indices]\n",
    "h_values     = [h_values[i]     for i in valid_h_indices]\n",
    "\n",
    "# Filter for daily data\n",
    "valid_d_indices = [i for i, k in enumerate(d_keys) if k in daily_input_data]\n",
    "d_keys       = [d_keys[i]       for i in valid_d_indices]\n",
    "d_agentsIDs  = [d_agentsIDs[i]  for i in valid_d_indices]\n",
    "d_values     = [d_values[i]     for i in valid_d_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5f79ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tf_loader(data_dict, time_steps, features):\n",
    "    def _load_sample(key, task_id, y):\n",
    "        k = key.numpy().decode(\"utf-8\")\n",
    "        x = data_dict[k].astype(\"float32\")   # [time_steps, features]\n",
    "        return x, task_id, y\n",
    "\n",
    "    def _tf_load(key, task_id, y):\n",
    "        x, t, yy = tf.py_function(\n",
    "            func=_load_sample,\n",
    "            inp=[key, task_id, y],\n",
    "            Tout=[tf.float32, tf.int32, tf.float32]\n",
    "        )\n",
    "        # set static shapes so batching works\n",
    "        x.set_shape([time_steps, features])\n",
    "        t.set_shape([])\n",
    "        yy.set_shape([])\n",
    "        return x, t, yy\n",
    "\n",
    "    return _tf_load\n",
    "\n",
    "hourly_input_shape = (list(hourly_input_data.values())[0]).shape\n",
    "daily_input_shape =  (list(daily_input_data.values())[0]).shape\n",
    "h_loader = make_tf_loader(hourly_input_data, *hourly_input_shape)\n",
    "d_loader = make_tf_loader(daily_input_data, *daily_input_shape)\n",
    "\n",
    "hourly_dataset = tf.data.Dataset.from_tensor_slices((h_keys, h_agentsIDs, h_values))\n",
    "daily_dataset = tf.data.Dataset.from_tensor_slices((d_keys, d_agentsIDs, d_values))\n",
    "\n",
    "hourly_dataset = hourly_dataset.map(h_loader, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "daily_dataset = daily_dataset.map(d_loader, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb83ab",
   "metadata": {},
   "source": [
    "- As we said earlier, the model should be able to use the station and date to access the actual training sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c3545299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MultiAgentModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,        # e.g. (time_steps, features)\n",
    "        num_outputs,          # integer: how many different heads/tasks\n",
    "        lstm_units=(128,64),\n",
    "        head_hidden_units=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # 1) Shared LSTM encoder\n",
    "        self.encoder = tf.keras.Sequential(name=\"encoder\")\n",
    "        self.encoder.add(tf.keras.layers.Masking(input_shape=input_shape,\n",
    "                                                 name=\"masking\"))\n",
    "        for i, units in enumerate(lstm_units):\n",
    "            return_seq = (i < len(lstm_units)-1)\n",
    "            self.encoder.add(tf.keras.layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=return_seq,\n",
    "                name=f\"lstm_{i}\"\n",
    "            ))\n",
    "\n",
    "        # 2) Build all regression heads, then fuse into one big Dense\n",
    "        #    We'll concatenate them so we end up with a [B, num_outputs] output.\n",
    "        self.heads = []\n",
    "        for tid in range(num_outputs):\n",
    "            head = tf.keras.Sequential(name=f\"head_{tid}\")\n",
    "            head.add(tf.keras.layers.InputLayer(input_shape=(lstm_units[-1],)))\n",
    "            head.add(tf.keras.layers.Dense(head_hidden_units,\n",
    "                                           activation=\"relu\",\n",
    "                                           name=\"dense_hidden\"))\n",
    "            head.add(tf.keras.layers.Dense(1,\n",
    "                                           activation=None,\n",
    "                                           name=\"dense_output\"))\n",
    "            self.heads.append(head)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x, task_ids = inputs\n",
    "        # 1) shared embedding\n",
    "        embs = self.encoder(x, training=training)      # [B, emb_dim]\n",
    "\n",
    "        # 2) run every head in parallel → list of [B,1]\n",
    "        out_per_task = [h(embs, training=training) for h in self.heads]\n",
    "        # 3) concat to [B, num_outputs]\n",
    "        all_preds = tf.concat(out_per_task, axis=-1)\n",
    "\n",
    "        # 4) pick each row’s column given task_ids\n",
    "        #    build indices [[0,tid0],[1,tid1],...]\n",
    "        batch_idx = tf.range(tf.shape(task_ids)[0])\n",
    "        idx = tf.stack([batch_idx, task_ids], axis=1)  # [B,2]\n",
    "        # 5) gather the correct prediction for each sample → [B]\n",
    "        return tf.gather_nd(all_preds, idx)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x_batch, task_ids, y_batch = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self((x_batch, task_ids), training=True)  # [B]\n",
    "            loss  = self.compiled_loss(y_batch, preds)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        # update metrics (if any)\n",
    "        self.compiled_metrics.update_state(y_batch, preds)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a404416",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(ds, test_frac=0.15, val_frac=0.1, batch_size=32):\n",
    "    total_size = len(list(ds))  # you can also use len(h_keys) or h_ds_raw.cardinality()\n",
    "    test_size  = int(test_frac * total_size)\n",
    "    val_size   = int(val_frac * (total_size - test_size))  # Val split after test\n",
    "    train_size = total_size - test_size - val_size\n",
    "\n",
    "    # Split into test and train\n",
    "    test_ds = ds.take(test_size)\n",
    "    train_ds = ds.skip(test_size)\n",
    "\n",
    "    # Shuffle the train set\n",
    "    train_ds = train_ds.shuffle(train_size, seed=42)\n",
    "\n",
    "    # Now split train into train and validation\n",
    "    val_ds = train_ds.take(val_size)\n",
    "    train_ds = train_ds.skip(val_size)\n",
    "\n",
    "    return (\n",
    "        train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE),\n",
    "        test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3fe492b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "h_train, h_val, h_test = split_dataset(hourly_dataset, batch_size=batch_size)\n",
    "d_train, d_val, d_test = split_dataset(daily_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb10552",
   "metadata": {},
   "source": [
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ba274ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = MultiAgentModel(\n",
    "    input_shape=daily_input_shape,\n",
    "    num_outputs=len(agents),\n",
    "    lstm_units=(128, 64),\n",
    "    head_hidden_units=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1377969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 61/268 [=====>........................] - ETA: 6s - loss: nan - mae: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m d_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,       \u001b[38;5;66;03m# mean squared error for regression\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# mean absolute error\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m d_hist \u001b[38;5;241m=\u001b[39m \u001b[43md_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m plot_history(d_hist)\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\GitHub\\AII-project\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",       # mean squared error for regression\n",
    "    metrics=[\"mae\"],  # mean absolute error\n",
    ")\n",
    "\n",
    "d_hist = d_model.fit(\n",
    "    x=d_train,\n",
    "    validation_data=d_val,\n",
    "    epochs=5\n",
    ")\n",
    "plot_history(d_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
