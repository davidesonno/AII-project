{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically download all the datasets needed\n",
    "\n",
    "You can use this notebook to customize and download automatically the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests -q\n",
    "%pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder='./datasets'\n",
    "\n",
    "# Pollution\n",
    "pollution_dates='2019-01-01','2024-12-31' # start,end; yyyy-mm-dd;\n",
    "pollution_links = [\n",
    "    f'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/dati-centraline-bologna-storico/exports/csv?lang=it&qv1=(data_inizio%3A%5B{pollution_dates[0]}T23%3A00%3A00Z%20TO%20{pollution_dates[1]}T22%3A59%3A59Z%5D)&timezone=Europe%2FRome&use_labels=true&delimiter=%3B'\n",
    "]\n",
    "\n",
    "# Traffic\n",
    "traffic_years = 2019,2020,2021,2022,2023,2024\n",
    "reading_link_before_2022 = 'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-autoveicoli-tramite-spire-anno-{year}/exports/csv?lang=it&timezone=Europe%2FRome&use_labels=true&delimiter=%3B'\n",
    "reading_link_from_2022 = 'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-{year}/exports/csv?lang=it&timezone=Europe%2FRome&use_labels=true&delimiter=%3B'\n",
    "accuracy_link = 'https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/accuratezza-spire-anno-{year}/exports/csv?lang=it&timezone=Europe%2FRome&use_labels=true&delimiter=%3B'\n",
    "reading_links = [reading_link_before_2022.format(year=year) if year<2022 else reading_link_from_2022.format(year=year) for year in traffic_years]\n",
    "accuracy_links = [accuracy_link.format(year=year) for year in traffic_years]\n",
    "\n",
    "# Weather\n",
    "weather_years = 2019,2020,2021,2022,2023,2024\n",
    "weather_link = 'https://dati-simc.arpae.it/opendata/erg5v2/timeseries/01421/01421_{year}.zip'\n",
    "weather_links = [weather_link.format(year=year) for year in weather_years]\n",
    "\n",
    "download_data = { # the keys are folder names. If the value is a dict it is a subfolder, else the list of files to download.\n",
    "    'pollution': pollution_links,\n",
    "    'traffic': {\n",
    "        'readings': reading_links,\n",
    "        'accuracies': accuracy_links\n",
    "        },\n",
    "    'weather': weather_links\n",
    "}\n",
    "\n",
    "file_names = {\n",
    "    'pollution': ['pollution.csv'],\n",
    "    'traffic': {\n",
    "        'readings': [f'{year}_traffic_reading.csv' for year in traffic_years],\n",
    "        'accuracies': [f'{year}_traffic_accuracy.csv' for year in traffic_years]\n",
    "        },\n",
    "    'weather': [f'{year}_weather.zip' for year in weather_years]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, folder, filename):\n",
    "    \"\"\"Download a file from the given URL and save it in the specified folder with the given filename.\"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)  # Create folder if it does not exist\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    \n",
    "    # Start downloading with a progress bar using tqdm\n",
    "    response = requests.get(url, stream=True)  # Use stream to avoid loading the entire file in memory at once\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        # Use tqdm for the progress bar\n",
    "        with open(file_path, 'wb') as file, tqdm(\n",
    "            desc=f\"Downloading {filename}\",\n",
    "            total=total_size, \n",
    "            unit='B', \n",
    "            unit_scale=True, \n",
    "            ncols=100\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(chunk_size=1024):\n",
    "                bar.update(len(data))  # Update the progress bar\n",
    "                file.write(data)\n",
    "        print(f\"File downloaded: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url} (Status code: {response.status_code})\")\n",
    "\n",
    "def download_files(download_data, file_names, base_folder):\n",
    "    \"\"\"Download files based on the download_data and file_names dictionaries.\"\"\"\n",
    "    for category, links_or_subfolders in download_data.items():\n",
    "        category_folder = os.path.join(base_folder, category)\n",
    "        \n",
    "        # Handle subfolder structure\n",
    "        if isinstance(links_or_subfolders, dict):\n",
    "            for subfolder, links in links_or_subfolders.items():\n",
    "                subfolder_name = os.path.join(category_folder, subfolder)\n",
    "                for url, filename in zip(links, file_names[category][subfolder]):\n",
    "                    if not os.path.exists(os.path.join(subfolder_name, filename)):\n",
    "                        download_file(url, subfolder_name, filename)\n",
    "                    else: \n",
    "                        print(f\"File already exists: {filename}\")\n",
    "        else:\n",
    "            for url, filename in zip(links_or_subfolders, file_names[category]):\n",
    "                if not os.path.exists(os.path.join(category_folder, os.path.basename(filename).split('.')[0]+'.csv')): # some files can be zips but w elook for csv\n",
    "                    download_file(url, category_folder, filename)\n",
    "                else: \n",
    "                    print(f\"File already exists: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pollution.csv: 47.4MB [03:08, 251kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/pollution/pollution.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2019_traffic_reading.csv: 77.1MB [01:23, 927kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2019_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2020_traffic_reading.csv: 76.1MB [01:25, 895kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2020_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2021_traffic_reading.csv: 78.1MB [01:14, 1.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2021_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2022_traffic_reading.csv: 81.8MB [01:13, 1.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2022_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2023_traffic_reading.csv: 84.6MB [02:10, 648kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2023_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2024_traffic_reading.csv: 83.2MB [01:35, 871kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/readings/2024_traffic_reading.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2019_traffic_accuracy.csv: 40.5MB [00:51, 793kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2019_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2020_traffic_accuracy.csv: 41.7MB [00:45, 909kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2020_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2021_traffic_accuracy.csv: 42.4MB [00:40, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2021_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2022_traffic_accuracy.csv: 44.7MB [00:50, 885kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2022_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2023_traffic_accuracy.csv: 47.2MB [00:55, 858kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2023_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2024_traffic_accuracy.csv: 46.6MB [00:46, 995kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/traffic/accuracies/2024_traffic_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2019_weather.zip: 100%|███████████████████████████████| 135k/135k [00:00<00:00, 239kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2019_weather.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2020_weather.zip: 100%|███████████████████████████████| 134k/134k [00:00<00:00, 234kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2020_weather.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2021_weather.zip: 100%|███████████████████████████████| 134k/134k [00:00<00:00, 239kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2021_weather.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2022_weather.zip: 100%|███████████████████████████████| 135k/135k [00:00<00:00, 176kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2022_weather.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2023_weather.zip: 100%|███████████████████████████████| 136k/136k [00:00<00:00, 245kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2023_weather.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2024_weather.zip: 100%|███████████████████████████████| 134k/134k [00:00<00:00, 238kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: .\\datasets/weather/2024_weather.zip\n"
     ]
    }
   ],
   "source": [
    "download_files(download_data, file_names, download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract zip archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather downloads are zip files containing dayly data and hourly data. We only want to keep the hourly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_rename_zip(zip_path, weather_folder, keep_zips=True):\n",
    "    \"\"\"Extract the ZIP file, keep only the CSV file ending with 'h', rename it, and move the ZIP to a zip folder.\"\"\"\n",
    "    temp_folder = os.path.join(weather_folder, 'temp')\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    # Open and extract files, then close ZIP before moving it\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_folder)\n",
    "        extracted_files = zip_ref.namelist()  # Get list of extracted files\n",
    "\n",
    "    # Find the CSV file that ends with 'h'\n",
    "    hourly_file = None\n",
    "    for file in extracted_files:\n",
    "        if file.endswith('h.csv'):\n",
    "            hourly_file = os.path.join(temp_folder, file)\n",
    "            break\n",
    "\n",
    "    if hourly_file:\n",
    "        # Create a new filename based on the original ZIP file name (without the .zip extension)\n",
    "        new_filename = f\"{os.path.splitext(os.path.basename(zip_path))[0]}.csv\"\n",
    "        new_file_path = os.path.join(weather_folder, new_filename)\n",
    "\n",
    "        # Rename the hourly CSV file to the new name\n",
    "        os.rename(hourly_file, new_file_path)\n",
    "        print(f\"Renamed and saved: {new_file_path}\")\n",
    "    else:\n",
    "        print(f\"No hourly CSV found in {zip_path}\")\n",
    "\n",
    "    # Handle ZIP file based on `keep_zips`\n",
    "    if keep_zips:\n",
    "        zip_folder = os.path.join(weather_folder, 'zip')\n",
    "        os.makedirs(zip_folder, exist_ok=True)\n",
    "        shutil.move(zip_path, os.path.join(zip_folder, os.path.basename(zip_path)))\n",
    "        print(f\"Moved {zip_path} to {zip_folder}\")\n",
    "    else:\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Deleted {zip_path}\")\n",
    "\n",
    "    # Clean up: Remove the temporary folder\n",
    "    shutil.rmtree(temp_folder, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed and saved: .\\datasets/weather/2023_weather.csv\n",
      "Deleted .\\datasets/weather/2023_weather.zip\n",
      "Renamed and saved: .\\datasets/weather/2022_weather.csv\n",
      "Deleted .\\datasets/weather/2022_weather.zip\n",
      "Renamed and saved: .\\datasets/weather/2021_weather.csv\n",
      "Deleted .\\datasets/weather/2021_weather.zip\n",
      "Renamed and saved: .\\datasets/weather/2024_weather.csv\n",
      "Deleted .\\datasets/weather/2024_weather.zip\n",
      "Renamed and saved: .\\datasets/weather/2019_weather.csv\n",
      "Deleted .\\datasets/weather/2019_weather.zip\n",
      "Renamed and saved: .\\datasets/weather/2020_weather.csv\n",
      "Deleted .\\datasets/weather/2020_weather.zip\n"
     ]
    }
   ],
   "source": [
    "weather_folder = os.path.join(download_folder, 'weather')\n",
    "\n",
    "for zip_file in os.listdir(weather_folder):\n",
    "    zip_path = os.path.join(weather_folder, zip_file)\n",
    "    if os.path.isfile(zip_path) and zip_file.endswith('.zip'):\n",
    "        extract_and_rename_zip(zip_path, weather_folder, keep_zips=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
